apiVersion: v1
data:
  bootstrap.sh: |
    #!/bin/bash -x
    echo Starting
    : ${HADOOP_HOME:=/opt/hadoop}
    echo Using ${HADOOP_HOME} as HADOOP_HOME
    . $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    CONFIG_DIR="/tmp/hadoop-config"
    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_HOME/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done
    cd $HADOOP_HOME/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -
    if [[ "${HOSTNAME}" =~ "hdfs-namenode-0" ]]; then
    #apt update && apt install psmisc && apt -y install openssh-server && service ssh start
      echo ${HOSTNAME}
      mkdir -p /data/hdfs/namenode
      if [ ! -f /data/hdfs/namenode/formated ]; then
        $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive && echo 1 > /data/hdfs/namenode/formated
      fi
      $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start namenode
      if [ ! -f /data/hdfs/namenode/formatZK ]; then
        $HADOOP_HOME/bin/hdfs zkfc -formatZK && echo 1 > /data/hdfs/namenode/formatZK
      fi
        $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start zkfc
    fi
    if [[ "${HOSTNAME}" =~ "hdfs-namenode-1" ]]; then
    #apt update && apt install psmisc && apt -y install openssh-server && service ssh start
      mkdir -p /data/hdfs/namenode
      if [ ! -f /data/hdfs/namenode/bootstrapStandbyd ]; then
        $HADOOP_HOME/bin/hdfs --loglevel INFO namenode -bootstrapStandby && echo 1 > /data/hdfs/namenode/bootstrapStandbyd
      fi
      $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start namenode
      $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start zkfc
    fi
    if [[ "${HOSTNAME}" =~ "hdfs-jn" ]]; then
      mkdir -p /data/journalnode
      $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start journalnode
    fi
    if [[ "${HOSTNAME}" =~ "hdfs-datanode" ]]; then
      mkdir -p /data/hdfs/datanode
      TMP_URL="http://hadoop-hdfs-namenode:9870"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        $HADOOP_HOME/bin/hdfs --loglevel INFO --daemon start datanode
      else
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi
    fi
    if [[ "${HOSTNAME}" =~ "yarn-resourcemanager" ]]; then
      $HADOOP_HOME/bin/yarn --loglevel INFO --daemon start resourcemanager
      $HADOOP_HOME/bin/yarn --loglevel INFO --daemon start proxyserver
    fi
    if [[ "${HOSTNAME}" =~ "yarn-nodemanager" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_HOME/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>
      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
      TMP_URL="http://hadoop-yarn-resourcemanager:8088/ws/v1/cluster/info"
      if timeout 5m bash -c "until curl -sf $TMP_URL; do echo Waiting for $TMP_URL; sleep 5; done"; then
        $HADOOP_HOME/bin/yarn nodemanager --loglevel INFO
      else
        echo "$0: Timeout waiting for $TMP_URL, exiting."
        exit 1
      fi
    fi
    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_HOME}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_HOME}/logs/* &
      while true; do sleep 1000; done
    fi
    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://linkflow</value>
          <description>NameNode URI</description>
      </property>
      <property>
          <name>ha.zookeeper.quorum</name>
          <value>zookeeper:2181</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.groups</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.anonymous.hosts</name>
          <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.anonymous.groups</name>
          <value>*</value>
      </property>
    </configuration>
  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.permissions.superusergroup</name>
        <value>root</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.datanode.hostname</name>
        <value>example.com</value>
      </property>
      <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:51000</value>
      </property>
      <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:50500</value>
      </property>
      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///data/hdfs/datanode</value>
        <description>DataNode directory</description>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///data/hdfs/namenode</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>
      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>
      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->
      <property>
        <name>dfs.nameservices</name>
        <value>linkflow</value>
      </property>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://linkflow</value>
      </property>
      <property>
        <name>dfs.ha.namenodes.linkflow</name>
        <value>nn1,nn2</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.linkflow.nn1</name>
        <value>hadoop-hdfs-namenode-0.hadoop-hdfs-namenode.linkflow.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.linkflow.nn2</name>
        <value>hadoop-hdfs-namenode-0.hadoop-hdfs-namenode.linkflow.svc.cluster.local:8020</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.linkflow.nn1</name>
        <value>hadoop-hdfs-namenode-0.hadoop-hdfs-namenode.linkflow.svc.cluster.local:8082</value>
      </property>
      <property>
        <name>dfs.namenode.rpc-address.linkflow.nn2</name>
        <value>hadoop-hdfs-namenode-1.hadoop-hdfs-namenode.linkflow.svc.cluster.local:8082</value>
      </property>
      <property>
        <name>dfs.client.failover.proxy.provider.linkflow</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
      </property>
      <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/data/journalnode</value>
      </property>
      <!-- 说明：namenode间用于共享编辑日志的journal节点列表 -->
      <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <!-- <value>qjournal://local-168-182-110:8485;local-168-182-111:8485;local-168-182-112:8485/myhdfs<</value> -->
        <value>qjournal://hdfs-jn-0.hdfs-jn.linkflow.svc.cluster.local:8485;hdfs-jn-1.hdfs-jn.linkflow.svc.cluster.local:8485;hdfs-jn-2.hdfs-jn.linkflow.svc.cluster.local:8485/linkflow</value>
      </property>
      <property>
        <name>dfs.ha.nn.not-become-active-in-safemode</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
      </property>

      <!-- 指定上述选项ssh通讯使用的密钥文件在系统中的位置 -->
      <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
      </property>
    </configuration>
  hive-site.xml: |
    <configuration>
      <property>
        <name>atlas.hook.hive.maxThreads</name>
        <value>1</value>
      </property>
      <property>
        <name>atlas.hook.hive.minThreads</name>
        <value>false</value>
      </property>
      <property>
        <name>datanucleus.autoCreateSchema</name>
        <value>false</value>
      </property>
      <property>
        <name>datanucleus.cache.level2.type</name>
        <value>none</value>
      </property>
      <property>
        <name>datanucleus.fixedDatastore</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.auto.convert.join</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.auto.convert.join.noconditionaltask</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.auto.convert.join.noconditionaltask.size</name>
        <value>1145324612</value>
      </property>
      <property>
        <name>hive.auto.convert.sortmerge.join</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.auto.convert.sortmerge.join.to.mapjoin</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.cbo.enable</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.cli.print.header</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.compactor.abortedtxn.threshold</name>
        <value>1000</value>
      </property>
      <property>
        <name>hive.compactor.check.interval</name>
        <value>300L</value>
      </property>
      <property>
        <name>hive.compactor.delta.num.threshold</name>
        <value>10</value>
      </property>
      <property>
        <name>hive.compactor.delta.pct.threshold</name>
        <value>0.1f</value>
      </property>
      <property>
        <name>hive.compactor.initiator.on</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.compactor.worker.threads</name>
        <value>0</value>
      </property>
      <property>
        <name>hive.compactor.worker.timeout</name>
        <value>86400L</value>
      </property>
      <property>
        <name>hive.compute.query.using.stats</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.conf.restricted.list</name>
        <value>hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role</value>
      </property>
      <property>
        <name>hive.default.fileformat</name>
        <value>TextFile</value>
      </property>
      <property>
        <name>hive.default.fileformat.managed</name>
        <value>TextFile</value>
      </property>
      <property>
        <name>hive.enforce.bucketing</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.enforce.sorting</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.enforce.sortmergebucketmapjoin</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.exec.compress.intermediate</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.exec.compress.output</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>strict</value>
      </property>
      <property>
        <name>hive.exec.failure.hooks</name>
        <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
      </property>
      <property>
        <name>hive.exec.max.created.files</name>
        <value>100000</value>
      </property>
      <property>
        <name>hive.exec.max.dynamic.partitions</name>
        <value>5000</value>
      </property>
      <property>
        <name>hive.exec.max.dynamic.partitions.pernode</name>
        <value>2000</value>
      </property>
      <property>
        <name>hive.exec.orc.compression.strategy</name>
        <value>SPEED</value>
      </property>
      <property>
        <name>hive.exec.orc.default.compress</name>
        <value>ZLIB</value>
      </property>
      <property>
        <name>hive.exec.orc.default.stripe.size</name>
        <value>67108864</value>
      </property>
      <property>
        <name>hive.exec.orc.encoding.strategy</name>
        <value>SPEED</value>
      </property>
      <property>
        <name>hive.exec.parallel</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.exec.parallel.thread.number</name>
        <value>8</value>
      </property>
      <property>
        <name>hive.exec.post.hooks</name>
        <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
      </property>
      <property>
        <name>hive.exec.pre.hooks</name>
        <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
      </property>
      <property>
        <name>hive.exec.reducers.bytes.per.reducer</name>
        <value>67108864</value>
      </property>
      <property>
        <name>hive.exec.reducers.max</name>
        <value>1009</value>
      </property>
      <property>
        <name>hive.exec.scratchdir</name>
        <value>/tmp/hive</value>
      </property>
      <property>
        <name>hive.exec.submit.local.task.via.child</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.exec.submitviachild</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.fetch.task.aggr</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.fetch.task.conversion</name>
        <value>more</value>
      </property>
      <property>
        <name>hive.fetch.task.conversion.threshold</name>
        <value>1073741824</value>
      </property>
      <property>
        <name>hive.limit.optimize.enable</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.limit.pushdown.memory.usage</name>
        <value>0.04</value>
      </property>
      <property>
        <name>hive.map.aggr</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.map.aggr.hash.force.flush.memory.threshold</name>
        <value>0.9</value>
      </property>
      <property>
        <name>hive.map.aggr.hash.min.reduction</name>
        <value>0.5</value>
      </property>
      <property>
        <name>hive.map.aggr.hash.percentmemory</name>
        <value>0.5</value>
      </property>
      <property>
        <name>hive.mapjoin.bucket.cache.size</name>
        <value>10000</value>
      </property>
      <property>
        <name>hive.mapjoin.optimized.hashtable</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.mapred.reduce.tasks.speculative.execution</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.merge.mapfiles</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.merge.mapredfiles</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.merge.orcfile.stripe.level</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.merge.rcfile.block.level</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.merge.size.per.task</name>
        <value>256000000</value>
      </property>
      <property>
        <name>hive.merge.smallfiles.avgsize</name>
        <value>16000000</value>
      </property>
      <property>
        <name>hive.metastore.authorization.storage.checks</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.metastore.cache.pinobjtypes</name>
        <value>Table,Database,Type,FieldSchema,Order</value>
      </property>
      <property>
        <name>hive.metastore.client.connect.retry.delay</name>
        <value>5s</value>
      </property>
      <property>
        <name>hive.metastore.client.socket.timeout</name>
        <value>1800s</value>
      </property>
      <property>
        <name>hive.metastore.connect.retries</name>
        <value>24</value>
      </property>
      <property>
        <name>hive.metastore.execute.setugi</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.metastore.failure.retries</name>
        <value>24</value>
      </property>
      <property>
        <name>hive.metastore.pre.event.listeners</name>
        <value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</value>
      </property>
      <property>
        <name>hive.metastore.sasl.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.metastore.server.max.threads</name>
        <value>100000</value>
      </property>
      <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hive-metastore:9083</value>
      </property>
      <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/apps/hive/warehouse</value>
      </property>
      <property>
        <name>hive.optimize.bucketmapjoin</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.bucketmapjoin.sortedmerge</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.optimize.constant.propagation</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.index.filter</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.metadataonly</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.null.scan</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.reducededuplication</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.optimize.reducededuplication.min.reducer</name>
        <value>4</value>
      </property>
      <property>
        <name>hive.optimize.sort.dynamic.partition</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.orc.compute.splits.num.threads</name>
        <value>10</value>
      </property>
      <property>
        <name>hive.orc.splits.include.file.footer</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.prewarm.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.prewarm.numcontainers</name>
        <value>3</value>
      </property>
      <property>
        <name>hive.security.authenticator.manager</name>
        <value>org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator</value>
      </property>
      <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.security.authorization.manager</name>
        <value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory</value>
      </property>
      <property>
        <name>hive.security.metastore.authenticator.manager</name>
        <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>
      </property>
      <property>
        <name>hive.security.metastore.authorization.auth.reads</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.security.metastore.authorization.manager</name>
        <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
      </property>
      <property>
        <name>hive.server2.allow.user.substitution</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.server2.authentication</name>
        <value>NONE</value>
      </property>
      <property>
        <name>hive.server2.authentication.spnego.keytab</name>
        <value>HTTP/_HOST@EXAMPLE.COM</value>
      </property>
      <property>
        <name>hive.server2.enable.doAs</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.server2.logging.operation.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.server2.logging.operation.log.location</name>
        <value>/tmp/hive/operation_logs</value>
      </property>
      <property>
        <name>hive.server2.max.start.attempts</name>
        <value>5</value>
      </property>
      <property>
        <name>hive.server2.table.type.mapping</name>
        <value>CLASSIC</value>
      </property>
      <property>
        <name>hive.server2.thrift.http.path</name>
        <value>cliservice</value>
      </property>
      <property>
        <name>hive.server2.thrift.http.port</name>
        <value>10001</value>
      </property>
      <property>
        <name>hive.server2.thrift.max.worker.threads</name>
        <value>500</value>
      </property>
      <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
      </property>
      <property>
        <name>hive.server2.thrift.sasl.qop</name>
        <value>auth</value>
      </property>
      <property>
        <name>hive.server2.transport.mode</name>
        <value>binary</value>
      </property>
      <property>
        <name>hive.server2.use.SSL</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.smbjoin.cache.rows</name>
        <value>10000</value>
      </property>
      <property>
        <name>hive.start.cleanup.scratchdir</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.stats.autogather</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.stats.dbclass</name>
        <value>fs</value>
      </property>
      <property>
        <name>hive.stats.fetch.column.stats</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.stats.fetch.partition.stats</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.support.concurrency</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.txn.manager</name>
        <value>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</value>
      </property>
      <property>
        <name>hive.txn.max.open.batch</name>
        <value>1000</value>
      </property>
      <property>
        <name>hive.txn.timeout</name>
        <value>300</value>
      </property>
      <property>
        <name>hive.user.install.directory</name>
        <value>/user/</value>
      </property>
      <property>
        <name>hive.vectorized.execution.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>hive.vectorized.execution.reduce.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>hive.vectorized.groupby.checkinterval</name>
        <value>4096</value>
      </property>
      <property>
        <name>hive.vectorized.groupby.flush.percent</name>
        <value>0.1</value>
      </property>
      <property>
        <name>hive.vectorized.groupby.maxentries</name>
        <value>100000</value>
      </property>
      <property>
        <name>hive.warehouse.subdir.inherit.perms</name>
        <value>true</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://mysql:3306/hive?useSSL=false</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>Sjtu403c@#%</value>
      </property>
      <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
      </property>
    </configuration>
  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop-yarn-resourcemanager-0.hadoop-yarn-resourcemanager:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop-yarn-resourcemanager-0.hadoop-yarn-resourcemanager:19888</value>
      </property>
    </configuration>
  slaves: |
    localhost
  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop-yarn-resourcemanager</value>
      </property>
      <!-- Bind to all interfaces -->
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->
      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>
      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>
      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>
      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>
      <property>
        <name>yarn.application.classpath</name>
        <value>
          /opt/hadoop/etc/hadoop,
          /opt/hadoop/share/hadoop/common/*,
          /opt/hadoop/share/hadoop/common/lib/*,
          /opt/hadoop/share/hadoop/hdfs/*,
          /opt/hadoop/share/hadoop/hdfs/lib/*,
          /opt/hadoop/share/hadoop/mapreduce/*,
          /opt/hadoop/share/hadoop/mapreduce/lib/*,
          /opt/hadoop/share/hadoop/yarn/*,
          /opt/hadoop/share/hadoop/yarn/lib/*
        </value>
      </property>
      <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>resourceCluster</value>
      </property>
      <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
      </property>
      <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>hadoop-yarn-resourcemanager-0.hadoop-yarn-resourcemanager.linkflow.svc.cluster.local</value>
      </property>
      <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>hadoop-yarn-resourcemanager-1.hadoop-yarn-resourcemanager.linkflow.svc.cluster.local</value>
      </property>
      <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>hadoop-yarn-resourcemanager-0.hadoop-yarn-resourcemanager.linkflow.svc.cluster.local:8088</value>
      </property>
      <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>hadoop-yarn-resourcemanager-1.hadoop-yarn-resourcemanager.linkflow.svc.cluster.local:8088</value>
      </property>
      <property>
        <name>hadoop.zk.address</name>
        <value>zookeeper:2181</value>
      </property>
      <!--启用ResouerceManager重启的功能，默认为false-->
      <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
      </property>

      <!--用于ResouerceManager状态存储的类-->
      <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
      </property>
    </configuration>
kind: ConfigMap
metadata:
  labels:
    app: hadoop
  name: hadoop
